{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas_profiling as ppf\n",
    "# dataset.info()\n",
    "# report = ppf.ProfileReport(dataset[['Adj Close','Volume']])\n",
    "# report.to_file('report_AdjCloseVolume.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler ##数据归一化\n",
    "\n",
    "output_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,m,n):\n",
    "    '''\n",
    "    data: the dataframe of stock price\n",
    "    m: 前m天预测\n",
    "    n: 预测几天\n",
    "    '''\n",
    "    minMax = MinMaxScaler()    \n",
    "    data_transformed = minMax.fit_transform(data)\n",
    "\n",
    "    adj_close = data[\"Adj Close\"].tolist()\n",
    "    adj_volume = data_transformed[:,-2:]\n",
    "    #\n",
    "    res_X = []\n",
    "    res_y = []\n",
    "    \n",
    "    for i in range(0,len(adj_close)-m-n+1):\n",
    "        res_X.append(adj_volume[i:i+m])\n",
    "        res_y.append(adj_close[i+m:i+m+n])\n",
    "    return res_X,res_y,adj_volume[-m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/elvis/ITProjects/GitHub/PythonTask/100dayML/100_day_ml_py/ml_project/raw_price_train/1_r_price_train.csv',index_col='Date')\n",
    "dataset.index = pd.to_datetime(dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566, 14, 2)\n",
      "[112.24508700000001, 109.356125, 108.679771, 106.75698899999999, 107.57826999999999, 105.298019, 102.44767]\n"
     ]
    }
   ],
   "source": [
    "X, y, final_pred = preprocess(dataset,14,output_size)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7)\n",
    "print((np.array(X_train)).shape)\n",
    "# print((np.array(y_train)).shape)\n",
    "# print((np.array(X)).shape)\n",
    "# print((np.array(y)).shape)\n",
    "# print((np.array(y[-1])).shape)\n",
    "print(y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "tensor_x_train = torch.Tensor(X_train)\n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "\n",
    "tensor_x_test = torch.Tensor(X_test)\n",
    "tensor_y_test = torch.Tensor(y_test)\n",
    "\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_train_dataset = Data.TensorDataset(tensor_x_train, tensor_y_train)\n",
    "torch_test_dataset = Data.TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=torch_train_dataset,      # torch TensorDataset format\n",
    "    batch_size=100,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=torch_test_dataset,      # torch TensorDataset format\n",
    "    batch_size=100,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape ( None, 14, 2)\n",
    "            nn.Conv1d(\n",
    "                in_channels=14,      # input height\n",
    "                out_channels=16,    # n_filters\n",
    "                kernel_size=5,      # filter size\n",
    "                stride=1,           # filter movement/step\n",
    "                padding=2,      # 如果想要 con2d 出来的图片长宽没有变化,当 stride=1 padding=(kernel_size-1)/2\n",
    "            ),\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool1d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (batch_size, 16, 1)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "#             nn.Linear(256*190, 256),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(16,output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1ed = self.conv1(x)  # 展平多维的卷积图成 (batch_size, 16, 1)\n",
    "#         print('conv1ed size ',conv1ed.size())\n",
    "        conv1ed = conv1ed.view(conv1ed.size(0), -1)\n",
    "#         print('conv1ed viewed size',conv1ed.size())\n",
    "        output = self.out(conv1ed)\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(14, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_func = nn.MSELoss()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def train(model, loader, optimizer, loss_func):\n",
    "    model.train()\n",
    "\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "        # 训练的地方...\n",
    "        batch_x = batch_x.view(-1, 14, 2)              # reshape x to (batch, time_step, input_size)\n",
    "        batch_y = batch_y.view(-1,output_size)\n",
    "        output = model(batch_x)\n",
    "        loss = loss_func(output, batch_y)                   # cross entropy loss\n",
    "        \n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = mean_squared_error(batch_y.numpy().tolist(),output.detach().numpy())\n",
    "        \n",
    "    return loss.data.numpy(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不用优化器了\n",
    "def evaluate(model, loader, loss_func):\n",
    "    # 转成测试模式，冻结dropout层或其他层\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "\n",
    "            batch_x = batch_x.view(-1, 14, 2)              # reshape x to (batch, time_step, input_size)\n",
    "            batch_y = batch_y.view(-1,output_size)\n",
    "            output = model(batch_x)\n",
    "            loss = loss_func(output, batch_y)\n",
    "            acc = mean_squared_error(batch_y.numpy().tolist(),output.detach().numpy())\n",
    "\n",
    "        # 调回训练模式\n",
    "        model.train()\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train_loss:  7596.5405 | train_mse x:  7596.54031646347 | test_mse x:  7796.711301318047\n",
      "Epoch:  100 | train_loss:  604.98804 | train_mse x:  604.9880626870462 | test_mse x:  626.1775160397635\n",
      "Epoch:  200 | train_loss:  129.85454 | train_mse x:  129.85453121285104 | test_mse x:  129.47517788692963\n",
      "Epoch:  300 | train_loss:  107.002625 | train_mse x:  107.00262587543439 | test_mse x:  131.98492662412542\n",
      "Epoch:  400 | train_loss:  142.62404 | train_mse x:  142.62403500714632 | test_mse x:  120.2609931140856\n",
      "Epoch:  500 | train_loss:  115.90116 | train_mse x:  115.90115841546368 | test_mse x:  97.73263537003827\n",
      "Epoch:  600 | train_loss:  94.105385 | train_mse x:  94.10538662071824 | test_mse x:  95.90734004068695\n",
      "Epoch:  700 | train_loss:  95.53449 | train_mse x:  95.53449369819292 | test_mse x:  66.30507067050142\n",
      "Epoch:  800 | train_loss:  48.00814 | train_mse x:  48.008140478289 | test_mse x:  57.287027766373754\n",
      "Epoch:  900 | train_loss:  30.900488 | train_mse x:  30.90048874503284 | test_mse x:  37.71720272753925\n",
      "Epoch:  1000 | train_loss:  14.080675 | train_mse x:  14.080675189876741 | test_mse x:  25.31527305158235\n",
      "Epoch:  1100 | train_loss:  9.812273 | train_mse x:  9.812273056034897 | test_mse x:  12.816792342110528\n",
      "Epoch:  1200 | train_loss:  9.861612 | train_mse x:  9.861612765003702 | test_mse x:  11.075749844812654\n",
      "Epoch:  1300 | train_loss:  9.992414 | train_mse x:  9.99241446952195 | test_mse x:  9.08573029256767\n",
      "Epoch:  1400 | train_loss:  9.388537 | train_mse x:  9.388536963524812 | test_mse x:  9.004234481271743\n",
      "Epoch:  1500 | train_loss:  9.791137 | train_mse x:  9.791136908194334 | test_mse x:  6.814686483755429\n"
     ]
    }
   ],
   "source": [
    "best_test_mse = float('inf')\n",
    "for epoch in range(1501):   # 训练所有!整套!数据 10 次\n",
    "    train_loss, train_mse = train(model, train_loader, optimizer, loss_func)\n",
    "    test_mse = evaluate(model, test_loader, loss_func)\n",
    "    \n",
    "    \n",
    "    if test_mse < best_test_mse:\n",
    "        best_test_mse = test_mse\n",
    "        torch.save(model.state_dict(), 'stock-cnn-model.pt')\n",
    "        \n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch: ', epoch, '| train_loss: ', train_loss, '| train_mse x: ', train_mse, '| test_mse x: ', test_mse)\n",
    "        # 打出来一些数据\n",
    "#         print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',\n",
    "#               batch_x.numpy(), '| batch y: ', batch_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([107.7120, 107.7185, 107.7346, 107.7106, 107.5657, 107.6164, 107.5342],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('stock-cnn-model.pt'))\n",
    "pred = model(torch.Tensor(final_pred).view(1,14,2))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
