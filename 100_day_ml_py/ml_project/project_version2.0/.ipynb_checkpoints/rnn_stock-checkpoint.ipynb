{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas_profiling as ppf\n",
    "# dataset.info()\n",
    "# report = ppf.ProfileReport(dataset[['Adj Close','Volume']])\n",
    "# report.to_file('report_AdjCloseVolume.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler ##数据归一化\n",
    "\n",
    "output_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,m,n):\n",
    "    '''\n",
    "    data: the dataframe of stock price\n",
    "    m: 前m天预测\n",
    "    n: 预测几天\n",
    "    '''\n",
    "    minMax = MinMaxScaler()    \n",
    "    data_transformed = minMax.fit_transform(data)\n",
    "\n",
    "    adj_close = data[\"Adj Close\"].tolist()\n",
    "    adj_volume = data_transformed[:,-2:]\n",
    "    #\n",
    "    res_X = []\n",
    "    res_y = []\n",
    "    \n",
    "    for i in range(0,len(adj_close)-m-n+1):\n",
    "        res_X.append(adj_volume[i:i+m])\n",
    "        res_y.append(adj_close[i+m:i+m+n])\n",
    "    return res_X,res_y,adj_volume[-m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/elvis/ITProjects/GitHub/PythonTask/100dayML/100_day_ml_py/ml_project/raw_price_train/1_r_price_train.csv',index_col='Date')\n",
    "dataset.index = pd.to_datetime(dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 2)\n"
     ]
    }
   ],
   "source": [
    "X, y, final_pred = preprocess(dataset,14,output_size)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7)\n",
    "# print((np.array(X_train)).shape)\n",
    "# print((np.array(y_train)).shape)\n",
    "# print((np.array(X)).shape)\n",
    "# print((np.array(y)).shape)\n",
    "# print((np.array(y[-1])).shape)\n",
    "print(np.array(final_pred).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "tensor_x_train = torch.Tensor(X_train)\n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "\n",
    "tensor_x_test = torch.Tensor(X_test)\n",
    "tensor_y_test = torch.Tensor(y_test)\n",
    "\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_train_dataset = Data.TensorDataset(tensor_x_train, tensor_y_train)\n",
    "torch_test_dataset = Data.TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=torch_train_dataset,      # torch TensorDataset format\n",
    "    batch_size=100,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=torch_test_dataset,      # torch TensorDataset format\n",
    "    batch_size=100,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 14, 2])\n",
      "torch.Size([100, 14, 2])\n",
      "torch.Size([100, 14, 2])\n",
      "torch.Size([100, 14, 2])\n",
      "torch.Size([100, 14, 2])\n",
      "torch.Size([66, 14, 2])\n"
     ]
    }
   ],
   "source": [
    "for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "    print(batch_x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(  # if use nn.RNN(), it hardly learns\n",
    "            input_size=2,\n",
    "            hidden_size=64,         # rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "#         print('r_out[:, -1, :]',r_out[:, -1, :].size())\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "class BILSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=2,\n",
    "            hidden_size=64,         # rnn hidden unit\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.2)\n",
    "        self.fc = nn.Sequential(\n",
    "#             nn.Linear(64*2, 32),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(64*2,output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.bilstm(x, None)   # None represents zero initial hidden state\n",
    "#         print('r_out',r_out.size())\n",
    "#         print('h_n',h_n.size())\n",
    "#         print('h_n[-2, :, :]',h_n[-2, :, :].size())\n",
    "#         print('h_n[-2, :, :],h_n[-1, :, :]),dim=1:',torch.cat((h_n[-2, :, :],h_n[-1, :, :]),dim=1).size())\n",
    "        output = self.fc(torch.cat((h_n[-2, :, :],h_n[-1, :, :]),dim=1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BILSTM()\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def train(model, loader, optimizer, loss_func):\n",
    "    model.train()\n",
    "\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "        # 训练的地方...\n",
    "        batch_x = batch_x.view(-1, 14, 2)              # reshape x to (batch, time_step, input_size)\n",
    "        batch_y = batch_y.view(-1,output_size)\n",
    "        output = model(batch_x)\n",
    "        loss = loss_func(output, batch_y)                   # cross entropy loss\n",
    "        \n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = mean_squared_error(batch_y.numpy().tolist(),output.detach().numpy())\n",
    "        \n",
    "    return loss.data.numpy(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不用优化器了\n",
    "def evaluate(model, loader, loss_func):\n",
    "    # 转成测试模式，冻结dropout层或其他层\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "\n",
    "            batch_x = batch_x.view(-1, 14, 2)              # reshape x to (batch, time_step, input_size)\n",
    "            batch_y = batch_y.view(-1,output_size)\n",
    "            output = model(batch_x)\n",
    "            loss = loss_func(output, batch_y)\n",
    "            acc = mean_squared_error(batch_y.numpy().tolist(),output.detach().numpy())\n",
    "\n",
    "        # 调回训练模式\n",
    "        model.train()\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | train_loss:  1.0793341 | train_mse x:  1.0793340979147126 | test_mse x:  5.08605900586121\n",
      "Epoch:  2 | train_loss:  0.86512715 | train_mse x:  0.8651271472026172 | test_mse x:  3.7112915856301822\n",
      "Epoch:  3 | train_loss:  0.987288 | train_mse x:  0.9872879806732558 | test_mse x:  1.7705096206845827\n",
      "Epoch:  4 | train_loss:  1.1776162 | train_mse x:  1.177616202949548 | test_mse x:  4.9356660207053205\n",
      "Epoch:  5 | train_loss:  1.0497309 | train_mse x:  1.049730915724605 | test_mse x:  4.40104284277355\n",
      "Epoch:  6 | train_loss:  1.0617707 | train_mse x:  1.061770731538952 | test_mse x:  3.488451821844264\n",
      "Epoch:  7 | train_loss:  1.0516388 | train_mse x:  1.0516388668032117 | test_mse x:  2.2061509402963178\n",
      "Epoch:  8 | train_loss:  0.8874944 | train_mse x:  0.8874943569010325 | test_mse x:  2.770129374686441\n",
      "Epoch:  9 | train_loss:  0.89888406 | train_mse x:  0.8988840632060696 | test_mse x:  4.445307321153892\n"
     ]
    }
   ],
   "source": [
    "best_test_mse = float('inf')\n",
    "for epoch in range(10):   # 训练所有!整套!数据 10 次\n",
    "    train_loss, train_mse = train(model, train_loader, optimizer, loss_func)\n",
    "    test_mse = evaluate(model, test_loader, loss_func)\n",
    "    \n",
    "    \n",
    "    if test_mse < best_test_mse:\n",
    "        best_test_mse = test_mse\n",
    "        torch.save(model.state_dict(), 'stock-bilstm-model.pt')\n",
    "        \n",
    "    if epoch> 0 :\n",
    "        print('Epoch: ', epoch, '| train_loss: ', train_loss, '| train_mse x: ', train_mse, '| test_mse x: ', test_mse)\n",
    "        # 打出来一些数据\n",
    "#         print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',\n",
    "#               batch_x.numpy(), '| batch y: ', batch_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[106.6203, 106.3472, 105.8384, 106.2928, 107.1255, 107.6579, 107.7180]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('stock-bilstm-model.pt'))\n",
    "pred = model(torch.Tensor(final_pred).view(1,14,2))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
