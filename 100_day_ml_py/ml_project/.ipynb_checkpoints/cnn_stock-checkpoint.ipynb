{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas_profiling as ppf\n",
    "# dataset.info()\n",
    "# report = ppf.ProfileReport(dataset[['Adj Close','Volume']])\n",
    "# report.to_file('report_AdjCloseVolume.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler ##数据归一化\n",
    "\n",
    "output_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,m,n):\n",
    "    '''\n",
    "    data: the dataframe of stock price\n",
    "    m: 前m天预测\n",
    "    n: 预测几天\n",
    "    '''\n",
    "    minMax = MinMaxScaler()    \n",
    "    data_transformed = minMax.fit_transform(data)\n",
    "\n",
    "    adj_close = data[\"Adj Close\"].tolist()\n",
    "    adj_volume = data_transformed[:,-2:]\n",
    "    #\n",
    "    res_X = []\n",
    "    res_y = []\n",
    "    \n",
    "    for i in range(0,len(adj_close)-m-n+1):\n",
    "        res_X.append(adj_volume[i:i+m])\n",
    "        res_y.append(adj_close[i+m:i+m+n])\n",
    "    return res_X,res_y,adj_volume[-m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/elvis/ITProjects/GitHub/PythonTask/100dayML/100_day_ml_py/ml_project/raw_price_train/1_r_price_train.csv',index_col='Date')\n",
    "dataset.index = pd.to_datetime(dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 2)\n"
     ]
    }
   ],
   "source": [
    "X, y, final_pred = preprocess(dataset,14,output_size)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7)\n",
    "# print((np.array(X_train)).shape)\n",
    "# print((np.array(y_train)).shape)\n",
    "# print((np.array(X)).shape)\n",
    "# print((np.array(y)).shape)\n",
    "# print((np.array(y[-1])).shape)\n",
    "print(np.array(final_pred).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "tensor_x_train = torch.Tensor(X_train)\n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "\n",
    "tensor_x_test = torch.Tensor(X_test)\n",
    "tensor_y_test = torch.Tensor(y_test)\n",
    "\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_train_dataset = Data.TensorDataset(tensor_x_train, tensor_y_train)\n",
    "torch_test_dataset = Data.TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=torch_train_dataset,      # torch TensorDataset format\n",
    "    batch_size=100,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=torch_test_dataset,      # torch TensorDataset format\n",
    "    batch_size=100,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(  # if use nn.RNN(), it hardly learns\n",
    "            input_size=2,\n",
    "            hidden_size=64,         # rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self,input_dim, hidden_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim,\n",
    "#                             num_layers=2,\n",
    "#                             bidirectional=True,\n",
    "#                             dropout=0.2)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim*2, 256),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256,output_dim),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_x):\n",
    "#         packed_output, (hidden, cell) = self.lstm(input_x)\n",
    "#         output = self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)).squeeze()\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def train(model, loader, optimizer, loss_func):\n",
    "    model.train()\n",
    "\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "        # 训练的地方...\n",
    "        batch_x = batch_x.view(-1, 14, 2)              # reshape x to (batch, time_step, input_size)\n",
    "        batch_y = batch_y.view(-1,output_size)\n",
    "        output = model(batch_x)\n",
    "        loss = loss_func(output, batch_y)                   # cross entropy loss\n",
    "        \n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = mean_squared_error(batch_y.numpy().tolist(),output.detach().numpy())\n",
    "        \n",
    "    return loss.data.numpy(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不用优化器了\n",
    "def evaluate(model, loader, loss_func):\n",
    "    # 转成测试模式，冻结dropout层或其他层\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "\n",
    "            batch_x = batch_x.view(-1, 14, 2)              # reshape x to (batch, time_step, input_size)\n",
    "            batch_y = batch_y.view(-1,output_size)\n",
    "            output = model(batch_x)\n",
    "            loss = loss_func(output, batch_y)\n",
    "            acc = mean_squared_error(batch_y.numpy().tolist(),output.detach().numpy())\n",
    "\n",
    "        # 调回训练模式\n",
    "        model.train()\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | train_loss:  11.796122 | train_mse x:  11.79612117488328 | test_mse x:  2488.969576258042\n",
      "Epoch:  2 | train_loss:  324.24832 | train_mse x:  324.2483088612124 | test_mse x:  1241.9258909912812\n",
      "Epoch:  3 | train_loss:  656.0684 | train_mse x:  656.0684117541781 | test_mse x:  764.5643059673437\n",
      "Epoch:  4 | train_loss:  826.8253 | train_mse x:  826.8252935553153 | test_mse x:  601.309569075371\n",
      "Epoch:  5 | train_loss:  887.5052 | train_mse x:  887.5051777762294 | test_mse x:  551.7441642053309\n",
      "Epoch:  6 | train_loss:  904.67487 | train_mse x:  904.674895784011 | test_mse x:  538.3671784469708\n",
      "Epoch:  7 | train_loss:  842.45764 | train_mse x:  842.4576624083649 | test_mse x:  512.6873593684203\n",
      "Epoch:  8 | train_loss:  2.704268 | train_mse x:  2.704267954759416 | test_mse x:  200.54787047120877\n",
      "Epoch:  9 | train_loss:  9.654967 | train_mse x:  9.654967494028304 | test_mse x:  37.945989440203576\n",
      "Epoch:  10 | train_loss:  14.383919 | train_mse x:  14.383918483244738 | test_mse x:  4.080410481280913\n",
      "Epoch:  11 | train_loss:  2.3382447 | train_mse x:  2.338244622057703 | test_mse x:  10.45738737289295\n",
      "Epoch:  12 | train_loss:  7.2575903 | train_mse x:  7.257590137644521 | test_mse x:  17.27582984093377\n",
      "Epoch:  13 | train_loss:  14.528366 | train_mse x:  14.528366236891348 | test_mse x:  6.5639562377140726\n",
      "Epoch:  14 | train_loss:  8.22503 | train_mse x:  8.225030164976488 | test_mse x:  31.248095190784493\n",
      "Epoch:  15 | train_loss:  1.6733434 | train_mse x:  1.6733434181618836 | test_mse x:  48.44077924299719\n",
      "Epoch:  16 | train_loss:  3.1851957 | train_mse x:  3.1851956851647367 | test_mse x:  43.25708192749049\n",
      "Epoch:  17 | train_loss:  6.004715 | train_mse x:  6.004715150594295 | test_mse x:  20.771474046258454\n",
      "Epoch:  18 | train_loss:  3.619147 | train_mse x:  3.6191470442198415 | test_mse x:  14.584261361118738\n",
      "Epoch:  19 | train_loss:  1.9833124 | train_mse x:  1.9833123882978856 | test_mse x:  7.19721620015168\n",
      "Epoch:  20 | train_loss:  12.053128 | train_mse x:  12.053128403393202 | test_mse x:  6.214547411646761\n",
      "Epoch:  21 | train_loss:  6.797797 | train_mse x:  6.797797094893342 | test_mse x:  4.961204639694188\n",
      "Epoch:  22 | train_loss:  3.7271578 | train_mse x:  3.7271577189655676 | test_mse x:  8.468120409475107\n",
      "Epoch:  23 | train_loss:  5.4466867 | train_mse x:  5.446686628657127 | test_mse x:  9.01411986817506\n",
      "Epoch:  24 | train_loss:  4.2587805 | train_mse x:  4.2587806574530465 | test_mse x:  10.210178257149112\n",
      "Epoch:  25 | train_loss:  3.3028967 | train_mse x:  3.3028967606868327 | test_mse x:  6.835572557582054\n",
      "Epoch:  26 | train_loss:  4.245716 | train_mse x:  4.245716160107155 | test_mse x:  7.846464826176608\n",
      "Epoch:  27 | train_loss:  6.8759546 | train_mse x:  6.875954578044392 | test_mse x:  8.359981663433635\n",
      "Epoch:  28 | train_loss:  5.8315253 | train_mse x:  5.831525439169907 | test_mse x:  8.972370106901508\n",
      "Epoch:  29 | train_loss:  5.123276 | train_mse x:  5.123276151136712 | test_mse x:  9.12414171578296\n",
      "Epoch:  30 | train_loss:  3.8934667 | train_mse x:  3.8934668193479802 | test_mse x:  8.755373782711104\n",
      "Epoch:  31 | train_loss:  7.6074567 | train_mse x:  7.6074565115517805 | test_mse x:  8.589301699107248\n",
      "Epoch:  32 | train_loss:  5.1734147 | train_mse x:  5.173414704371161 | test_mse x:  8.685079774212292\n",
      "Epoch:  33 | train_loss:  5.366153 | train_mse x:  5.366152867080278 | test_mse x:  12.25356631244566\n",
      "Epoch:  34 | train_loss:  16.228323 | train_mse x:  16.22832226328319 | test_mse x:  9.361844897394933\n",
      "Epoch:  35 | train_loss:  8.156494 | train_mse x:  8.156494512663423 | test_mse x:  9.451636663978986\n",
      "Epoch:  36 | train_loss:  5.74474 | train_mse x:  5.744740099207514 | test_mse x:  20.81781213624968\n",
      "Epoch:  37 | train_loss:  9.5873 | train_mse x:  9.587300218248856 | test_mse x:  10.886630817333103\n",
      "Epoch:  38 | train_loss:  6.3732557 | train_mse x:  6.3732558260651 | test_mse x:  10.114847764431033\n",
      "Epoch:  39 | train_loss:  5.9172873 | train_mse x:  5.917287349863078 | test_mse x:  13.3585872128689\n",
      "Epoch:  40 | train_loss:  6.647388 | train_mse x:  6.6473881878877625 | test_mse x:  9.930023396354434\n",
      "Epoch:  41 | train_loss:  9.415758 | train_mse x:  9.41575781967965 | test_mse x:  13.98242541842462\n",
      "Epoch:  42 | train_loss:  11.06146 | train_mse x:  11.06145991102468 | test_mse x:  23.910662813968624\n",
      "Epoch:  43 | train_loss:  6.8323617 | train_mse x:  6.832361731919393 | test_mse x:  12.210917588897116\n",
      "Epoch:  44 | train_loss:  8.834743 | train_mse x:  8.83474246100481 | test_mse x:  6.695891330908386\n",
      "Epoch:  45 | train_loss:  8.519711 | train_mse x:  8.519710369979814 | test_mse x:  12.350155535794329\n",
      "Epoch:  46 | train_loss:  12.531514 | train_mse x:  12.531514509104975 | test_mse x:  12.89865092049253\n",
      "Epoch:  47 | train_loss:  9.033167 | train_mse x:  9.033167270324027 | test_mse x:  13.603186479037893\n",
      "Epoch:  48 | train_loss:  8.124605 | train_mse x:  8.124605500180873 | test_mse x:  13.241307033783025\n",
      "Epoch:  49 | train_loss:  11.747843 | train_mse x:  11.747843190983986 | test_mse x:  6.486304311547428\n",
      "Epoch:  50 | train_loss:  12.254225 | train_mse x:  12.254224719236456 | test_mse x:  10.642507279724148\n",
      "Epoch:  51 | train_loss:  10.825989 | train_mse x:  10.825988709203168 | test_mse x:  11.149880142092504\n",
      "Epoch:  52 | train_loss:  9.970453 | train_mse x:  9.970453481189907 | test_mse x:  12.732010263708487\n",
      "Epoch:  53 | train_loss:  8.2668085 | train_mse x:  8.26680862125899 | test_mse x:  13.140509014658164\n",
      "Epoch:  54 | train_loss:  9.263952 | train_mse x:  9.263952169598529 | test_mse x:  14.410117672922622\n",
      "Epoch:  55 | train_loss:  8.311695 | train_mse x:  8.311695265290577 | test_mse x:  15.011632482845537\n",
      "Epoch:  56 | train_loss:  13.106729 | train_mse x:  13.106728590361724 | test_mse x:  18.24152134578409\n",
      "Epoch:  57 | train_loss:  12.768649 | train_mse x:  12.76864867202364 | test_mse x:  12.835218524668432\n",
      "Epoch:  58 | train_loss:  10.131245 | train_mse x:  10.131244389167737 | test_mse x:  17.071339338360954\n",
      "Epoch:  59 | train_loss:  10.005856 | train_mse x:  10.005855561982441 | test_mse x:  17.829678364488895\n",
      "Epoch:  60 | train_loss:  10.854982 | train_mse x:  10.854982203199013 | test_mse x:  20.840330261504278\n",
      "Epoch:  61 | train_loss:  12.13056 | train_mse x:  12.130559829132315 | test_mse x:  18.663105959330487\n",
      "Epoch:  62 | train_loss:  11.526222 | train_mse x:  11.52622260339974 | test_mse x:  25.57304102070962\n",
      "Epoch:  63 | train_loss:  10.906348 | train_mse x:  10.906348399117373 | test_mse x:  29.341901486764463\n",
      "Epoch:  64 | train_loss:  9.201304 | train_mse x:  9.201304105619784 | test_mse x:  30.404183456549486\n",
      "Epoch:  65 | train_loss:  8.7025585 | train_mse x:  8.702558739187745 | test_mse x:  28.463595175425457\n",
      "Epoch:  66 | train_loss:  8.5250025 | train_mse x:  8.525002125342976 | test_mse x:  26.520877914320277\n",
      "Epoch:  67 | train_loss:  11.536623 | train_mse x:  11.536622874380555 | test_mse x:  23.70295124146755\n",
      "Epoch:  68 | train_loss:  10.343728 | train_mse x:  10.343728348417374 | test_mse x:  27.171504831664997\n",
      "Epoch:  69 | train_loss:  9.850985 | train_mse x:  9.850984658174898 | test_mse x:  25.2709649261004\n",
      "Epoch:  70 | train_loss:  9.705645 | train_mse x:  9.705644435058016 | test_mse x:  20.76137009991466\n",
      "Epoch:  71 | train_loss:  10.686058 | train_mse x:  10.686058454417175 | test_mse x:  7.660424043790305\n",
      "Epoch:  72 | train_loss:  10.923631 | train_mse x:  10.923630299050794 | test_mse x:  19.534211151917198\n",
      "Epoch:  73 | train_loss:  10.768492 | train_mse x:  10.768492211045148 | test_mse x:  23.00458876418582\n",
      "Epoch:  74 | train_loss:  10.225017 | train_mse x:  10.22501680950102 | test_mse x:  22.98122431415998\n",
      "Epoch:  75 | train_loss:  10.188574 | train_mse x:  10.188574265511956 | test_mse x:  22.343912286002055\n",
      "Epoch:  76 | train_loss:  9.2004595 | train_mse x:  9.2004590694435 | test_mse x:  25.093816058136458\n",
      "Epoch:  77 | train_loss:  8.583471 | train_mse x:  8.583470905361796 | test_mse x:  24.398617395781912\n",
      "Epoch:  78 | train_loss:  8.099508 | train_mse x:  8.099507832201198 | test_mse x:  22.03358076535265\n",
      "Epoch:  79 | train_loss:  8.377295 | train_mse x:  8.377294530212696 | test_mse x:  22.68267350105868\n",
      "Epoch:  80 | train_loss:  9.970252 | train_mse x:  9.970251851580022 | test_mse x:  22.94317292370501\n",
      "Epoch:  81 | train_loss:  6.7768693 | train_mse x:  6.776869307862528 | test_mse x:  8.483592559982624\n",
      "Epoch:  82 | train_loss:  8.772375 | train_mse x:  8.772374651893708 | test_mse x:  27.12177523142392\n",
      "Epoch:  83 | train_loss:  8.240567 | train_mse x:  8.240567107256668 | test_mse x:  26.02625136775896\n",
      "Epoch:  84 | train_loss:  8.577796 | train_mse x:  8.577796093046866 | test_mse x:  19.96162523547121\n",
      "Epoch:  85 | train_loss:  9.30489 | train_mse x:  9.304889446280347 | test_mse x:  25.360733566647728\n",
      "Epoch:  86 | train_loss:  10.401823 | train_mse x:  10.40182288349024 | test_mse x:  28.738959159885002\n",
      "Epoch:  87 | train_loss:  11.015408 | train_mse x:  11.015407890353734 | test_mse x:  27.45653529759563\n",
      "Epoch:  88 | train_loss:  10.005674 | train_mse x:  10.005674659731865 | test_mse x:  27.121769571527174\n",
      "Epoch:  89 | train_loss:  9.0242405 | train_mse x:  9.024240680053481 | test_mse x:  22.14793203150787\n",
      "Epoch:  90 | train_loss:  8.323574 | train_mse x:  8.323574304898752 | test_mse x:  21.942710333162854\n",
      "Epoch:  91 | train_loss:  8.529642 | train_mse x:  8.529642075701434 | test_mse x:  23.512832482761173\n",
      "Epoch:  92 | train_loss:  8.304362 | train_mse x:  8.304362271577702 | test_mse x:  24.797905352698372\n",
      "Epoch:  93 | train_loss:  9.219566 | train_mse x:  9.219566501319475 | test_mse x:  23.194071345871116\n",
      "Epoch:  94 | train_loss:  9.295012 | train_mse x:  9.295012829428222 | test_mse x:  25.430404506083246\n",
      "Epoch:  95 | train_loss:  9.793886 | train_mse x:  9.793885865117772 | test_mse x:  20.675754899649682\n",
      "Epoch:  96 | train_loss:  9.765996 | train_mse x:  9.765996436551047 | test_mse x:  20.49214752516543\n",
      "Epoch:  97 | train_loss:  10.638678 | train_mse x:  10.638677727611919 | test_mse x:  19.294873130241676\n",
      "Epoch:  98 | train_loss:  9.376432 | train_mse x:  9.37643277744272 | test_mse x:  16.730916987431037\n",
      "Epoch:  99 | train_loss:  8.923657 | train_mse x:  8.923657548615925 | test_mse x:  15.220344532696929\n"
     ]
    }
   ],
   "source": [
    "best_test_mse = float('inf')\n",
    "for epoch in range(100):   # 训练所有!整套!数据 10 次\n",
    "    train_loss, train_mse = train(model, torch_train_dataset, optimizer, loss_func)\n",
    "    test_mse = evaluate(model, torch_test_dataset, loss_func)\n",
    "    \n",
    "    \n",
    "    if test_mse < best_test_mse:\n",
    "        best_test_mse = test_mse\n",
    "        torch.save(model.state_dict(), 'stock-rnn-model.pt')\n",
    "        \n",
    "    if epoch> 0 :\n",
    "        print('Epoch: ', epoch, '| train_loss: ', train_loss, '| train_mse x: ', train_mse, '| test_mse x: ', test_mse)\n",
    "        # 打出来一些数据\n",
    "#         print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',\n",
    "#               batch_x.numpy(), '| batch y: ', batch_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[108.5597, 108.6888, 108.4333, 108.5589, 108.5190, 108.4234, 108.4376]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('stock-rnn-model.pt'))\n",
    "pred = model(torch.Tensor(final_pred).view(1,14,2))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
