{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,m):\n",
    "    '''\n",
    "    data: the dataframe of stock price\n",
    "    m: the length of sequence\n",
    "    '''\n",
    "    adj_close = data[\"Adj Close\"].tolist()\n",
    "    adj_volume = np.array(dataset[[\"Adj Close\",\"Volume\"]]).tolist()\n",
    "    #\n",
    "    res_X = []\n",
    "    res_y = []\n",
    "    # 剔除前4个元素\n",
    "    for i in range(0,len(adj_close)-m):\n",
    "        res_X.append(adj_volume[i:i+m])\n",
    "        res_y.append(adj_close[i+m])\n",
    "    return res_X,res_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/elvis/ITProjects/GitHub/PythonTask/100dayML/100_day_ml_py/ml_project/raw_price_train/1_r_price_train.csv',index_col='Date')\n",
    "dataset.index = pd.to_datetime(dataset.index)\n",
    "# test_set = dataset['2015-12-12':'2015-12-20']\n",
    "# dataset = dataset['2012-01-01':'2015-12-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as ppf\n",
    "dataset.info()\n",
    "report = ppf.ProfileReport(dataset[['Adj Close','Volume']])\n",
    "report.to_file('report_AdjCloseVolume.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = dataset['2015-12-7':'2015-12-12']\n",
    "test_x_torch = torch.from_numpy(np.array(test_x[[\"Adj Close\",\"Volume\"]]).astype(np.float32))\n",
    "test_y_torch = torch.from_numpy(np.array(test_set[\"Adj Close\"]).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = preprocess(dataset, 5)\n",
    "after_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "x = torch.Tensor(X)\n",
    "y = torch.Tensor(Y)\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=100,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(  # if use nn.RNN(), it hardly learns\n",
    "            input_size=2,\n",
    "            hidden_size=64,         # rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self,input_dim, hidden_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim,\n",
    "#                             num_layers=2,\n",
    "#                             bidirectional=True,\n",
    "#                             dropout=0.2)\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim*2, 256),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256,output_dim),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_x):\n",
    "#         packed_output, (hidden, cell) = self.lstm(input_x)\n",
    "#         output = self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)).squeeze()\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):   # 训练所有!整套!数据 10 次\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "        # 训练的地方...\n",
    "        batch_x = batch_x.view(-1, 5, 2)              # reshape x to (batch, time_step, input_size)\n",
    "        batch_y = batch_y.view(-1,1)\n",
    "        output = model(batch_x)\n",
    "        loss = loss_func(output, batch_y)                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch > 990:\n",
    "            if step % 50 == 0:\n",
    "                test_output = model(test_x_torch.view(-1,5,2))                # (samples, time_step, input_size)\n",
    "#             accuracy = test_output.detach().numpy()/test_y_torch.detach().numpy()\n",
    "                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % test_output)\n",
    "\n",
    "        # 打出来一些数据\n",
    "#         print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',\n",
    "#               batch_x.numpy(), '| batch y: ', batch_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
